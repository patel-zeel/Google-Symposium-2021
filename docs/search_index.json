[["index.html", "Google Symposium GS 1 Preface", " Google Symposium Zeel B Patel 2021-04-10 GS 1 Preface Here I have put the detailed notes taken at Google symposium 2021. "],["day-1-7-april-2021.html", "GS 2 Day 1 (7-April-2021) 2.1 Opening Keynote by Jay Yagnik YouTube 9:00 AM 2.2 General ML session by Katherine Heller 10 AM 2.3 Panel Discussion on Academia and Industry Research Careers, 1:30 PM YouTube link", " GS 2 Day 1 (7-April-2021) 2.1 Opening Keynote by Jay Yagnik YouTube 9:00 AM Jay divided his presentation into three topics. 2.1.1 Patterns of progress in AI End-to-end ML techniques result in practically the best models in almost all the domains (e.g., health care, speech recognition, vision-based robotics, self-driving vehicles, games, agriculture, weather forecasting). Maybe it allows using some mathematical shortcuts underlying the phenomena. Reusable blocks of ML (e.g., Tensorflow, PyTorch) allow building upon it and quickly connecting with the other pipelines. Recurrence is closely related to the deep part of DL, yet it does not work well. This is a contradiction or a paradox Very little has changed in terms of fundamental blocks of DL. Researchers must look at this as there is much room for improvement at the fundamental levels. Another example of reusability is transfer learning using BERT models Model capacity and compute have grown way faster than Moore’s law. TPU is able to work fast because of lower precision computation Potential technology inflation point: Quantum Computing Quantum processors can execute exponentially complex tasks in linear time A challenge of noise exists but can be tackled in the future by a self-error-correcting mechanism using a large number of qubits. 2.1.2 Future of artificial general intelligence (AGI) Including world knowledge in models that are grounded in reality would work well. 2.1.3 Societal impact with AI Looking at the patient history and helping doctors to focus on important events of the past. Flood warning system by Google Sharing fields on which Google India is working on Google level as well as societal level. 2.1.4 Q&amp;A Q: How would one balance between breadth and depth as a PhD student? A: Jay said he started as a machine learning and vision guy 15 years back. After that, he forced himself to learn a new field at a complexity level of a practitioner of that field every two to three years. He felt that summarizing all the domains, the number of core problems being solved is really small. Just because of different names and slightly different ways, they look very different. Thus, he advised first to get grounded in one field at the nuts and bolts level, and then it would force one to make connections among various areas, which is also a good recipe for building a good career. Q: What are the expectations from a PhD student who wants to join the research industry? Where should one focus in the last years of PhD? How different is research in the industry from academia? A: Answering the last question first, industry focuses on four pillars (patterns observed in successful researches): i) fundamental research; ii) Building infrastructure; iii) Taking the product to larger user base; iv) new product renovation. Answering the other questions, one should be able to summarize what they know in different fields. Jay recalls his adviser’s advice that one should be able to summarize a paper in 3-5 sentences. Jay also builds a corollary on top of it that one should write papers one would not be able to summarize in 3-5 sentences. Q: How to explain end-to-end models? Does Google use something specifically for the explainability of models? A: Most models are black-box nowadays, and we also have black-box methods to probe them (pointing to gradient propagation and uncertainty propagation methods). What happens in between (exact mathematics) is of the least concern as long as one can reason the output based on particular inputs (e.g., which part of a medical image drove the decision made by an AI model?) Q: As the model parameters are growing into millions, we need to scale hardware (GPUs and TPUs) at the same pace. This might be a bottleneck after some time. What are your views on this? A: Jay has two answers to this question. First is, having a small GPU cluster at the university level is not that expensive, and it is enough to do meaningful experiments that can scale in practice. Secondly, Jay believes he would not be surprised if a few years from now we discover that we were unnecessarily wasting a lot of compute and there are clever mathematical tricks (nearest neighbor searching, branch and bound) to do the task efficiently. Q: Most ML models learn with back-propagation, but the human brain does not seem to learn that way. How to closely mimic human brain learning for AGI (artificial general intelligence)? A: Jay does not think it is a requirement for AGI (quoting airplanes don’t flap their wings philosophy). He feels that taking inspiration from nature is good, but one has to stay within the limits of what machines can do. 2.2 General ML session by Katherine Heller 10 AM Katherine talked elaborately about her research projects. 2.2.1 Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier Multitask Gaussian process models were used to convert irregularly spaced data into equally spaced time-series data in this work. After that, the RNN model was used to predict the probability of having sepsis on real-world data. Authors use the Lanczos method to cope with the Gaussian process’s time complexity and draw approximate samples. Authors were able to improve the performance of previous work significantly. 2.2.2 Graph-Coupled HMMs for Modeling the Spread of Infection Authors try to efficiently model the spread of infection with GCHMMs leveraging sparsity in social networks. Authors successfully leverage mobile phone data collected from 84 people over an extended period to model the spread of infection on an individual level 2.2.3 Hierarchical Graph-Coupled HMMs for Heterogeneous Personalized Health Data In this work, HGCHMMs were used to detect infections in a small mobile community. The model predicted the probability of infections for each person each day. 2.2.4 Useful resources Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges 2.3 Panel Discussion on Academia and Industry Research Careers, 1:30 PM YouTube link The discussion was moderated by Manish Gupta. The panel members were diverse, from well-established researchers to a final year PhD student. Manish started the discussion with a question, “How PhD has helped you in your career?”. Pankaj Jalote brought up three important points that he has used in all the things he did: i) lookup for existing work or literature; ii) talk to the people who know more than you; iii) think academic even in the industrial settings. He adds that in his time, he had not many choices but to go with industry. Arti Deo shared here the journey of encounter with neural networks in the first wave. According to her, a PhD trains one to thrive in ambiguity. She prefers industry over academia because she wanted to see the value created by her work. She also says that PhD helps in the industry because one goes through the same problem-solving journey as a PhD. Manish adds that he also valued the confidence one gets by going through this journey. Abhijnan Chakraborty emphasized the freedom one gets in academics by the example of neural networks, which lived on in academia without a real push from the industries. He also mentions that one can be in academia and stay engaged with industries, so there is no one v/s another choice. Preksha Nema shared her journey and shared how her way of approaching a problem has changed after doing a PhD. Manish mentioned an article in The Hindu by Pankaj on “What can India do to promote more quality Ph.D programs?” and asked Pankaj on sharing more views on the topic. Pankaj brought an insight from his experience that a fresh PhD graduate from India is not considered as good as candidates from a foreign university. Thus Indian PhD students need to get global exposure. 2.3.1 Q&amp;A Q: How should one decide between academic and industry research careers? How harder is it to move from one to another? A: Arati answers the first part by conveying that if one is interested in seeing the immediate value creation industry is a good option. But if one is willing to wait and choose from a broader list of topics without worrying about immediate outcomes, academia is better. Clarifying the second question, Pankaj says that to switch from industry to academia, keep publishing while in the industry. To do vice versa, be engaged in applied research. Q: What do you look for in a candidate in the selection process? A: Pankaj says that, for the assistant professor role, they prefer the high potential (how much one knows outside their area) despite having a low number of publications. Arati emphasizes problem-solving skills and thinking beyond narrow areas while having enough depth in the area of expertise. Abhijnan gives a candidate’s viewpoint, saying that he would look if he is comfortable with other team members. Preksha prefers a team that has overlapping interests with her work. 2.3.2 Closing Question Manish brings the session to a close-by seeking general advice for students from the panelists. Preksha recommends going for an internship in the industry. Abhijnan suggests that if one is confused between academia and industry after the internship, they may spend two years in either field after their PhD. Arati advises being passionate about work wherever one is. Pankaj believes that one exposure to academia and industry helps in both careers. So, one should definitely go for an internship in their PhD because they get academic exposure in the PhD. "],["day-2-8-april-2021.html", "GS 3 Day 2 (8-April-2021) 3.1 A general ML session bt Geoffrey Hinton 9 AM 3.2 A AI4SG session by Pradeep Varakantham 10:30 AM 3.3 A General ML Session by M Pawan and K Dvijotham, 2 PM", " GS 3 Day 2 (8-April-2021) 3.1 A general ML session bt Geoffrey Hinton 9 AM Prof. Geoffrey gave a talk on a non-working system called GLOM. It tries to solve the question “How can a neural network with a fixed architecture parse an image into a part-whole hierarchy?” Geoffrey believes that if GLOM can be made to work in neural networks and transformers, it will significantly improve interpretability. 3.2 A AI4SG session by Pradeep Varakantham 10:30 AM Pradeep talked about a non-trivial solution to an optimal supply-demand ecosystem that is not working based on greedy supply to the immediate demands (e.g., assigning the closest Uber taxi to a customer). He proposes a Resource-constrained RL model for the problem where several constraints specific to a problem are supplied to an RL algorithm. He called it ReCo-RL (Resource-constrained RL problem). Another applied approach is the Deep Q network. Pradeep is trying to solve problems related to Taxi fleets, Emergency Response, Traffic and Security Patrols, and, Bike-sharing systems. 3.3 A General ML Session by M Pawan and K Dvijotham, 2 PM The talk covers various techniques to prevent adversarial attacks on image classifiers. These techniques involve the verification of the neural networks trained on the datasets. 3.3.1 Additional resources Adversarial Robustness through Local Linearization This was a close to the common sessions, from tomorrow onwards, special topic sessions are delivered. I am in the core-ML track "],["day-3-9-april-2021.html", "GS 4 Day 3 (9-April-2021) 4.1 ML foundations by Ravi kumar 9 AM 4.2 ML foundations: Stochastic Gradient Descent (Theory v/s practice) by Prateek Jain, 10:30 AM 4.3 Panel discussion: AI in India, 1 PM YouTube link", " GS 4 Day 3 (9-April-2021) 4.1 ML foundations by Ravi kumar 9 AM Ravi talks about a generic framework for online learning. A simple problem to model in this framework can be taken as a ski-rental problem where renting cost is 1$, but ski cost is 25$. The algorithm tries to give theoretical bounds on the maximum cost involved before making a correct decision. Ravi also emphasizes that online learning is pessimistic opposite to machine learning which tries to learn average behavior. The key idea is to incorporate ‘hints’ at each time step before making a decision. The research is trying to achieve close to offline level performance when hints are good and ensure to perform as good as using no hints in the worst case when all the hints are bad. Ravi starts with a basic formulation of the problem and builds upon a story on how research has evolved at each stage to solve previous solutions’ issues. Worst case cost bounds are derived for each case by various researchers. In the end, the last case is covered where multiple hints are incorporated at a single time step. A minimal set of experiments were performed on the ResNet classifier, where a hint is previous gradient. I feel that we may try to model our online prediction models this way. 4.1.1 Additional resources Online Learning with Imperfect Hints Online Linear Optimization with Many Hints 4.2 ML foundations: Stochastic Gradient Descent (Theory v/s practice) by Prateek Jain, 10:30 AM First, Prateek shows how different the SGD is in practice than in theory in terms of convexity, non-i.i.d selection of samples, batch size, step size, etc. Then the discussion moves on to how fast gradient descent can find a p-order stationary point. It turns out that for \\(p\\ge4\\), finding stationary points is an NP-hard problem. Somehow, gradient descent can still find local optima, but the rates are unknown. Gradient descent can find the first-order stationary points. Computing hessian is expensive, and so is finding second-order stationary points. But, there is a simple algorithm to find second-order stationary points with gradients alone. It is noisy GD where you add an isotropic noise to the solution and re-do GD. Practically, it is believed that SGD is equivalent to this, and it finds second-order stationary points with higher probability. It is an open problem to show this concretely. Prateek concludes the talk by mentioning that if one analyses SGD, it might be possible to overcome the current problems and bring the convergence rate further down. 4.3 Panel discussion: AI in India, 1 PM YouTube link The discussion was led by Partha Talukdar from Google. The first question was about the journey of Dr. Geetha Manjunath from Niramai who has developed a successful AI solution for early detection of breast cancer. Geetha explains that X-ray-based cancer detection is not affordable to everyone, and the machines are not affordable by most hospitals. They were able to successfully leverage thermography to solve this problem and make the test affordable as low as 2$ in rural India. She also mentions that they have regulatory clearances by Indian and European standards. Dr. Balaraman Ravindran admires the end-to-end work done by Niramai, reminding that in India, most verticals do not have the base technology ready to put an AI on top of that. He briefly talks about the need for fairness in AI as per the Indian context. Pankaj Gupta, Sr. Director of Engineering at Google Pay, throws some insights on the need of connecting the majority of India digitally using AI. 4.3.1 Q&amp;A Q: What might be good sources to look at updates/problems tackled with AI in India by various industries? A: Partha says that a symposium like the current can be insightful for this. Geetha suggests looking at FDA-approved startups, and a website called IndiaAI. Ravi suggests an excellent website called AI4Bharat that is maintained specifically for this purpose. Pankaj strongly recommends Twitter, and according to him, many latest cutting-edge things are first shared on Twitter nowadays. Q: What AI work is happening in India to mitigate Air pollution? What are the potential directions with AI in this field? A: Ravi describes a bit on mobile sensors used in Tamilnadu to monitor air pollution. According to him, due to heterogeneity in AQI in Delhi, even regression modeling or time-series predictions also help understand the pollution. Pankaj suggests the cerca center in IIT Delhi and the work of Prof. Rijurekha. Q: How to take AI solution to the ground level where one has to deal with bureaucracy? A: Geetha suggests approaching negative feedback with a positive mindset and improve what one lacks in the implementation. Q: How PhD students can work with industries to solve problems with AI? A: Ravi mentions that there are limited people who can work with Google, Microsoft, so try to see if you can be competitive by creating a startup. Geetha resonates a bit by suggesting working with AI startups to solve challenging problems. 4.3.2 Final thoughts Geetha encourages all to work with a problem that one can relate to and go end-to-end without stopping at the algorithmic level only. Ravi suggests being aware of the area’s ecosystem where one might be trying to solve a problem. According to him, sometimes AI is consuming a tiny part of the entire problem, so be aware of it and understand it to be successful. Pankaj suggests not to get bogged down by the difficulties and be around the right people. 4.3.3 Websites and resources FDA approved algortihms AI for Bharat India AI CENTRE OF EXCELLENCE FOR RESEARCH ON CLEAN AIR "]]
