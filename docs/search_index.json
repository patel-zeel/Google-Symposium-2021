[["index.html", "Google Symposium GS 1 Preface", " Google Symposium Zeel B Patel 2021-04-10 GS 1 Preface Here I have put the detailed notes taken at Google symposium 2021. "],["day-1-7-april-2021.html", "GS 2 Day 1 (7-April-2021) 2.1 Opening Keynote by Jay Yagnik YouTube 9:00 AM 2.2 General ML session by Katherine Heller 10 AM 2.3 Panel Discussion on Academia and Industry Research Careers, 1:30 PM YouTube link", " GS 2 Day 1 (7-April-2021) 2.1 Opening Keynote by Jay Yagnik YouTube 9:00 AM Jay divided his presentation in three topics 2.1.1 Patterns of progress in AI End-to-end ML techniques result in practically best models in almost all the domains (e.g. heath care, speech recognition, vision based robotics, self-driving vehicles, games, agriculture, weather forecasting). Maybe it allows to use some mathematical shortcuts underlying the phenomena. Reusable blocks of ML (e.g. Tensorflow, PyTorch) allow to quickly build upon it and connect with the other pipe-lines. Recurrence is closely related to deep part of DL yet it does not work well. This is a contradiction or a paradox Very little has changed in terms of fundamental blocks of DL. Researchers must look at this as there is much room for improvement at the fundamental levels. Another example of reusability is transfer learning using BERT models Model capacity and compute has grown way faster than Moore’s law. TPU is able to work fast because of lower precision computation Potential technology inflation point: Quantum Computing Quantum processors can execute exponentially complex tasks in linear time A challenge of noise exists, but can be tackled in future by self-error-correcting mechanism using large number of qubits. 2.1.2 Future of artificial general intelligence (AGI) Including world knowledge in models that are grounded in reality would work well. 2.1.3 Societal impact with AI Looking at the patient history and helping doctors to focus on important events of past. Flood warning system by Google Sharing fields on which Google India is working on Google level as well as societal level. 2.1.4 Q&amp;A Q: How would one balance between breadth and depth as a PhD student? A: Jay told he started as a machine learning and vision guy 15 years back. After that, he forced himself every two-three years to learn a new field at a complexity level of practitioner of that field. He felt that summarizing all the domains, the number of core problems being solved are really small. Just because of different names and slightly different ways they look very different. Thus, he advised to first get grounded in one field at nuts and bolts level and then it would force one to make connections among various fields which is also a good recipe of building a good career. Q: What are the expectations from a PhD students who wants to join the research industry? Where should one focus in last years of PhD? How different is reasearch in industry than academia? A: Answering the last question first, industry focuses on four pillars (patterns observed in successful researches): i) fundamental research; ii) Building infrastructure; iii) Taking product to larger user base; iv) new product renovation. Answering the other questions, one should be able to summarize what they know in different fields. Jay recalls his adviser’s advise that one should be able to summarize a paper in 3-5 sentences. Jay also builds a corollary on top of it that one should write papers one would not be able to summarize in 3-5 sentences. Q: How to explain end-to-end models? Does Google use something specifically for explainability of models? A: Most models are black-box nowadays and we also have black-box methods to probe them (pointing to gradient propagation and uncertainty propagation methods). What happens in between (exact mathematics) is of least concern as long as one is able to reason the output based on particular inputs (e.g. which part of a medical image drove the decision made by an AI model?) Q: As the models parameters are growing into millions we need to scale hardwares (GPUs and TPUs) at the same pace. This might be bottleneck after some time. What are your views on this? A: Jay has two answers for this question. First is, having a small GPU cluster at university level is not that expensive and it is enough to do meaningful experiments that can scale in practice. Secondly, Jay believes he would not be surprised if few years from now we discover that we were unnecessarily wasting a lot of compute and there are clever mathematical tricks (nearest neighbor searching, branch and bound) to do the task efficiently. Q: Most ML models learn with back-propogation but human brain does not seem to learn that way. How to closely mimic human brain learning for AGI (artificial general intelligence)? A: Jay does not think it is an requirement for AGI (quoting airoplanes don’t flap their wings philosophy). He feels that taking inspiration from nature is good but one has to stay within the limits of what machines can do. 2.2 General ML session by Katherine Heller 10 AM Katherine talked elaborately about several research projects of her. 2.2.1 Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier In this work Multitask Gaussian process models were used to convert irregularly spaced data into equally spaced time-series data. After that, RNN model was used to predict the probability of having sepsis on a real world data. Authors use Lanczos method to cope up with time-complexity of Gaussian process and to draw approximate samples. Authors were able to improve the performance on previous work significantly. 2.2.2 Graph-Coupled HMMs for Modeling the Spread of Infection Authors try to efficiently model the spread of infection with GCHMMs leveraging sparsity in social networks. Authors successfully leverage mobile phone data collected from 84 people over an extended period of time to model the spread of infection on an individual level 2.2.3 Hierarchical Graph-Coupled HMMs for Heterogeneous Personalized Health Data In this work, HGCHMMs were used to detect infections in a small mobile community. The probability of infections were predicted for each person each day. 2.2.4 Useful resources Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges 2.3 Panel Discussion on Academia and Industry Research Careers, 1:30 PM YouTube link The discussion was moderated by Manish Gupta. The panel members were diverse from well-established researchers to a final year PhD student. Manish started the discussion with a question “How PhD has helped you in your career?”. Pankaj Jalote brought up three important points that he has used in all the things he did: i) look up for existing work or literature; ii) talk to the people who know more than you; iii) think academic even in the industrial settings. He adds that in his time he had not many choices but to go with industry. Arti Deo shared here journey of encounter with neural networks in the first wave. According to her, a PhD trains one to thrive in ambiguity. She prefers industry over academia because she wanted to see the value created by her work. She also says that PhD helps in industry because one goes through the same problem solving journey as one goes in a PhD. Manish adds that he also valued the confidence one gets by going through this journey. Abhijnan Chakraborty emphasized the freedom one gets in academics by example of neural networks, which lived on in academia without real push from the industries. He also mentions that one can be in academia and also stay engaged with industries so there is no one v/s other choice to make here. Preksha Nema shared her journey and told how her way of approaching a problem as changed after doing a PhD. Manish mentioned an article in The Hindu by Pankaj on “What can India do to promote more quality Ph.D programmes?” and asked Pankaj on sharing more views on the topic. Pankaj brought an insight from his experience that a fresh PhD graduate from India is not considered as good as candidates from foreign university and thus Indian PhD students need to get global exposure. 2.3.1 Q&amp;A Q: How should one decide between academic and industry research careers? How harder is it to move from one to another? A: Arati answers the first part by conveying that if one is interested in seeing the immediate value creation industry is a good option. But if one is willing to wait and choose from a broader list of topics without worrying about immediate outcomes, academia is better. Clarifying the second question, Pankaj says that to switch from industry to academia, keep publishing while in the industry. To do vice versa, be engaged in applied research. Q: What do you look for in a candidate in selection process? A: Pankaj says that, for assistant professor role, they prefer the high potential (how much one knows outside their area) despite having low number of publications. Arati emphasizes on the problem solving skills and thinking beyond narrow area at the same time having enough depth in the area of expertise. Abhijnan gives view point of a candidtate saying that he would look if he is confirtable with other team members. Preksha prefers a team which has overlapping interests with her work. 2.3.2 Closing Question Manish brings the session to a close by seeking general advice for students from the panelists. Preksha recommends to go for an internship in an industry. abhijnan suggests that if one is confused between academia and industry after the internship, they may spend 2 years in either field after their PhD. Arati suggests to be passionate about work wherever one is. Pankaj believes that one exposure to acadamia and industry helps in both careers. So, one shoud definitely go for an internship in their PhD because they are anyway getting academia exposure in the PhD. "],["day-2-8-april-2021.html", "GS 3 Day 2 (8-April-2021) 3.1 A general ML session bt Geoffrey Hinton 9 AM 3.2 A AI4SG session by Pradeep Varakantham 10:30 AM 3.3 A General ML Session by M Pawan and K Dvijotham, 2 PM", " GS 3 Day 2 (8-April-2021) 3.1 A general ML session bt Geoffrey Hinton 9 AM Prof. Geoffrey gave a talk on a non-working system called GLOM. It tries to solve the question “How can a neural network with a fixed architecture parse an image into a partwhole hierarchy?” Geoffrey believes that if GLOM can be made to work in neural networks and transformers, it will significantly improve the interpretability. 3.2 A AI4SG session by Pradeep Varakantham 10:30 AM Pradeep talked about a non-trivial solution to optimal supply-demand ecosystem which is not working based on greedy supply to the immediate demands (e.g. assigning closest Uber taxi to a customer). He proposes Resource constrained RL model for the problem where several constraints specific to a problem are supplied to an RL algorithm. He called it ReCo-RL (Resource constrained RL problem). Another applied approach is Deep Q network. Pradeep is trying to solve problems related to Taxi fleets, Emergency Response, Traffic and Security Patrols, and, Bike sharing systems. 3.3 A General ML Session by M Pawan and K Dvijotham, 2 PM The talk covers various techniques to prevent adversarial attacks on image classifiers. These techniques involve verification of the neural networks trained on the datasets. 3.3.1 Additional resources Adversarial Robustness through Local Linearization This was a close to the common sessions, from tomorrow onwards, special topic sessions are delivered. I am in the core-ML track "],["day-3-9-april-2021.html", "GS 4 Day 3 (9-April-2021) 4.1 ML foundations by Ravi kumar 9 AM 4.2 ML foundations: Stochastic Gradient Descent (Theory v/s practice) by Prateek Jain, 10:30 AM 4.3 Panel discussion: AI in India, 1 PM YouTube link", " GS 4 Day 3 (9-April-2021) 4.1 ML foundations by Ravi kumar 9 AM Ravi talks about a generic framework for online learning. A simple problem to model in this framework can be taken as ski-rental problem where renting cost is 1$ but ski cost is 25$. Algorithm tries to give theoretical bounds on maximum cost involved before making correct decision. Ravi also emphasizes that online learning is pessimistic opposted to machine learning which tries to learn average behavior. The key idea is to incorporate ‘hints’ at each time-step before making a decision. The research is trying to achieve close to offline level performance when hints are good but also ensure to perform as good as using no hints in worst case when all the hints are bad. Ravi starts with basic formulation of the problem and builds upon a story on how research has evolved at each stage to solve the problems of previous solutions. Worst case cost bounds are derived for each case by various researchers. At the end, final case is covered where multiple hints are incorporated at a single time-step. On the practical side, a minimal set of experiments were performed on resnet classifier where hint is previous gradient. I feel that we may try to model our online prediction models this way. 4.1.1 Additional resources Online Learning with Imperfect Hints Online Linear Optimization with Many Hints 4.2 ML foundations: Stochastic Gradient Descent (Theory v/s practice) by Prateek Jain, 10:30 AM First, Prateek shows how different is the SGD used in practice than in theory in terms of convexity, non i.i.d selection of samples, batch size, step size etc. Then the discussion moves on how fast gradient descent can find a p-order stationary point. It turns out that for \\(p\\ge4\\), finding stationary points is NP-hard problem. Somehow, gradient descent can still find local optima but the rates are unknown. Gradient descent can find the first order stationary points. Computing hessian is expensive and so is to find second order stationary points. But, there is a simple algorithm to find second order stationary point with gradients alone. It is noisy GD where you add an isotropic noise to solution and re-do GD. Practically, it is believed that SGD is equivalent to this and it finds second order stationary points with higher probability. It is an open problem to show this concretely. Prateek concludes the talk by mentioning that if one analyses SGD, it might be possible to overcome the current problems it has and also bring the convergence rate further down. 4.3 Panel discussion: AI in India, 1 PM YouTube link The discussion was led by Partha Talukdar from Google. The first question was about the journy of Dr. Geetha Manjunath from Niramai who has developed a successful AI solution for early detection of breast cancer. Geetha explains that X-ray based cancer detection is not affordable to everyone and the machines are not affordable by most of the hospitals. They were able to successfully leverage thermography to solve this problem and make the test affordable as low as 2$ in rural India. She also mentions that they have regulatory clearances by Indian and Europian standards. Dr. Balaraman Ravindran admires the end-to-end work done by Niramai reminding that in India, most verticals do not have the base technology ready to put an AI on top of that. Adding to that, he briefly talks about need of fairness in AI as per the Indian context. Pankaj Gupta, Sr. Director of Engineering at Google Pay, throws some insignts on need of connecting majority of India digitally using AI. 4.3.1 Q&amp;A Q: What might be good sources to look at updates/problems tackled with AI in India by various industries? A: Partha says that a symposium like the current can be insightful for this. Geetha suggests to look at FDA approved startups and a website called IndiaAI. Ravi suggests a very good website called AI4Bharat that is maintained specifically for this purpose. Pankaj strongly recommends Twitter and according to him, many latest cutting-edge things are first shared on Twitter nowadays. Q: What AI work is happending in India to mitigate Air pollution? What are potential directions with AI in this field? A: Ravi describes a bit on mobile sensors used in Tamilnadu to monitor air pollution. According to him, due to heterogenety in AQI in Delhi, even regression modeling or time-series predictions are also of great help in understanding the pollution. Pankaj suggests cerca center in IIT Delhi and work of Prof. Rijurekha. Q: How to take AI solution to the ground level where one has to deal with bureaucracy? A: Geetha suggests to approach negative feedback with positive mindset and improve what one really lacks in the implementation. Q: How PhD students can work with industries to solve problems with AI? A: Ravi mentions that there are really limited people who can work with Google, Microsoft like compnies so try to see if you can be competitive by creating a stratup. Geetha resonates a bit by suggesting to work with AI startups to solve real hard problems. 4.3.2 Final thoughts Geetha encourgues all to work with a problem that one can relate with and go end-to-end without stopping at algorithmic level only. Ravi suggests to be aware of the ecosystem of area where one might be trying to solve a problem. According to him, sometimes AI is consuming really small part of the entire problem so be aware of it and understand it to be successful. Pankaj suggests not to get bogged down by the difficulties and be around the right people. 4.3.3 Websites and resources FDA approved algortihms AI for Bharat India AI CENTRE OF EXCELLENCE FOR RESEARCH ON CLEAN AIR "]]
